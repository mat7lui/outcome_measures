{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_return(import_file_location):\n",
    "    \n",
    "    if import_file_location.split('.')[1] == 'csv':\n",
    "        data = pd.read_csv(import_file_location)\n",
    "    else:\n",
    "        data = pd.read_excel(import_file_location)\n",
    "\n",
    "    data.drop(labels=[\n",
    "        'Respondent ID', 'Collector ID', 'Start Date', 'End Date', \n",
    "        'IP Address', 'Email Address', 'First Name', 'Last Name', 'Custom Data 1', \n",
    "        'Program'], axis=1, inplace=True)\n",
    "    data.drop(0, inplace=True)\n",
    "\n",
    "    new_cols = [\n",
    "        'first_name', 'last_name', 'assess_date', 'cottage', \n",
    "        'ders_1', 'ders_2', 'ders_3','ders_4','ders_5','ders_6','ders_7','ders_8','ders_9','ders_10','ders_11','ders_12','ders_13','ders_14','ders_15','ders_16',\n",
    "        'ari_1', 'ari_2', 'ari_3', 'ari_4', 'ari_5', 'ari_6', 'ari_7',\n",
    "        'dts_1', 'dts_2', 'dts_3', 'dts_4', 'dts_5', 'dts_6', 'dts_7', 'dts_8', 'dts_9', 'dts_10', 'dts_11', 'dts_12', 'dts_13', 'dts_14', 'dts_15',\n",
    "        'ceas_self_1', 'ceas_self_2', 'ceas_self_3', 'ceas_self_4', 'ceas_self_5', 'ceas_self_6', 'ceas_self_7', 'ceas_self_8', 'ceas_self_9', 'ceas_self_10', 'ceas_self_11', 'ceas_self_12', 'ceas_self_13',\n",
    "        'ceas_from_1', 'ceas_from_2', 'ceas_from_3', 'ceas_from_4', 'ceas_from_5', 'ceas_from_6', 'ceas_from_7', 'ceas_from_8', 'ceas_from_9', 'ceas_from_10', 'ceas_from_11', 'ceas_from_12', 'ceas_from_13',\n",
    "        'ceas_to_1', 'ceas_to_2', 'ceas_to_3', 'ceas_to_4', 'ceas_to_5', 'ceas_to_6', 'ceas_to_7', 'ceas_to_8', 'ceas_to_9', 'ceas_to_10', 'ceas_to_11', 'ceas_to_12', 'ceas_to_13',\n",
    "        'camm_1', 'camm_2', 'camm_3', 'camm_4', 'camm_5', 'camm_6', 'camm_7', 'camm_8', 'camm_9', 'camm_10'\n",
    "    ]\n",
    "    \n",
    "    data.columns = new_cols\n",
    "    data.insert(loc=1, column='name', value=data['last_name'].str.strip(' ') + ',' + data['first_name'].str.strip(' '))\n",
    "    data.drop(['first_name', 'last_name', 'cottage'], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    data['assess_date'] = pd.to_datetime(data['assess_date'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_ders(dataframe):\n",
    "    '''\n",
    "    SCORING METHODOLOGY:\n",
    "    DERS score is the sum of all 16 questions\n",
    "    \n",
    "    SCORING DETAILS:\n",
    "    Range of possible scores: 16-80\n",
    "    Good score = LOWER\n",
    "    Bad score = HIGHER\n",
    "    '''\n",
    "    return dataframe.loc[:, [col for col in dataframe.columns if \"ders\" in col]].sum(axis='columns')\n",
    "\n",
    "def score_ari(dataframe):\n",
    "    '''\n",
    "    SCORING METHODOLOGY:\n",
    "    ARI score is the sum of the first 6 items. The final question \"Overall irritability\" is not scored.\n",
    "    \n",
    "    SCORING DETAILS:\n",
    "    Range of possible scores: 0-12\n",
    "    Good score = LOWER\n",
    "    Bad score = HIGHER\n",
    "    '''\n",
    "    return dataframe.loc[:, [col for col in dataframe.columns if \"ari\" in col]].iloc[:, :6].sum(axis='columns')\n",
    "\n",
    "def score_dts(dataframe):\n",
    "    '''\n",
    "    SCORING METHODOLOGY:\n",
    "    The DTS has 4 recognized subscales:\n",
    "        - Tolerance - ability to tolerate emotions \n",
    "                QUESTIONS(1,3,5)\n",
    "        - Appraisal - assessment of the emotional situation as acceptable \n",
    "                QUESTIONS(6*,7,9,10,11,12) \n",
    "        - Absorption - level of attention absorbed by the negative emotion and relevant interference with functioning \n",
    "                QUESTIONS(2,4,15)\n",
    "        - Regulation - ability to regulate emotion \n",
    "                QUESTIONS(8,13,14)\n",
    "    Scores from each subscale are valid and can be calculated by taking the average of each question in the subscale\n",
    "    The overall DTS score is calculated by taking the average of all the subscale scores.\n",
    "    \n",
    "    SCORING DETAILS:\n",
    "    Range of all possible scores: \n",
    "        1-5, as a floating-point value\n",
    "    Good score = HIGHER\n",
    "    Bad score = LOWER\n",
    "    \n",
    "    * Question 6 is REVERSE scored.\n",
    "    '''\n",
    "    tolerance = ['dts_1', 'dts_3', 'dts_5']\n",
    "    appraisal = ['dts_6', 'dts_7', 'dts_9', 'dts_10', 'dts_11', 'dts_12']\n",
    "    absorption = ['dts_2', 'dts_4', 'dts_15']\n",
    "    regulation = ['dts_8', 'dts_13', 'dts_14']\n",
    "\n",
    "    tolerance_score = dataframe.loc[:, tolerance].mean(axis='columns')\n",
    "    appraisal_score = dataframe.loc[:, appraisal].mean(axis='columns')\n",
    "    absorption_score = dataframe.loc[:, absorption].mean(axis='columns')\n",
    "    regulation_score = dataframe.loc[:, regulation].mean(axis='columns')\n",
    "    overall_score = (tolerance_score + appraisal_score + absorption_score + regulation_score) / 4\n",
    "    return overall_score\n",
    "\n",
    "def score_ceas(dataframe):\n",
    "    '''\n",
    "    SCORING METHODOLOGY:\n",
    "    Within each component of the CEAS (Self-Compassion, Compassion TOWARDS others, Compassion FROM others), there are two separate domains:\n",
    "        - Engagement QUESTIONS(1,2,4,5,6,8)\n",
    "        - Action QUESTIONS(9,10,12,13)\n",
    "    These two domains are scored separately (QUESTIONS 3, 7, and 11 are not included in scoring) and the component scores are derived from \n",
    "    the sum of the respective Engagement & Action scales. \n",
    "    \n",
    "    SCORING DETAILS:\n",
    "    Range of possible scores: \n",
    "        Engagement = 6-60 \n",
    "        Action = 4-40\n",
    "        Component-level = 10-100    \n",
    "    '''\n",
    "    ceas = dataframe.loc[:, [col for col in dataframe.columns if \"ceas_\" in col]]\n",
    "    cols_to_drop = ['ceas_self_3', 'ceas_self_7', 'ceas_self_11',\n",
    "          'ceas_to_3', 'ceas_to_7', 'ceas_to_11', \n",
    "          'ceas_from_3', 'ceas_from_7', 'ceas_from_11']\n",
    "    ceas.drop(labels=cols_to_drop, axis='columns', inplace=True)\n",
    "    ceas_self = ceas.loc[:, [col for col in ceas.columns if \"self\" in col]].sum(axis=1)\n",
    "    ceas_to = ceas.loc[:, [col for col in ceas.columns if \"to\" in col]].sum(axis=1)\n",
    "    ceas_from = ceas.loc[:, [col for col in ceas.columns if \"from\" in col]].sum(axis=1)\n",
    "    return (ceas_self, ceas_to, ceas_from)\n",
    "\n",
    "def score_camm(dataframe):\n",
    "    '''\n",
    "    SCORING METHODOLGY:\n",
    "    CAMM score is simply the sum of all questions* on the scale. \n",
    "    \n",
    "    SCORING DETAILS:\n",
    "    Range of possible scores: 0-40\n",
    "    \n",
    "    * All questions on the CAMM are reverse scored\n",
    "    '''\n",
    "    return dataframe.loc[:, [col for col in dataframe.columns if \"camm\" in col]].sum(axis='columns')\n",
    "\n",
    "def generate_scores(path_to_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
